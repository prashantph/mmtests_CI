# This is a page allocator micro-benchmark implemented in systemtap. The
# performance of this is indirectly important for a number of workloads
# but bear in mind that the cost of allocation for many workloads is
# dominated by the cost of zeroing memory rather than anything the
# allocator itself is doing
#
# Copyright 2011 Mel Gorman <mgorman@suse.de>
%{
#include <linux/fs.h>
#include <linux/types.h>
#include <linux/proc_fs.h>
#include <linux/kernel.h>
#include <linux/vmalloc.h>
#include <linux/proc_fs.h>
#include <linux/seq_file.h>

#define PARAM_GFPFLAGS GFP_KERNEL
#define PARAM_ORDER 0
#define PARAM_ITERATIONS_INNER 10000
#define PARAM_ITERATIONS_OUTER 15
#define PARAM_BATCH 1

#ifdef CONFIG_X86
/**
 * rdtsc: Read the current number of clock cycles that have passed
 */
inline unsigned long long read_clockcycles(void)
{
	unsigned long low_time, high_time;
	asm volatile( 
		"rdtsc \n\t" 
			: "=a" (low_time),
			  "=d" (high_time));
        return ((unsigned long long)high_time << 32) | (low_time);
}
#else
inline unsigned long long read_clockcycles(void)
{
	return jiffies;
}
#endif /* CONFIG_X86 */

static int pagealloc_run(struct seq_file *m, void *v)
{
	unsigned int order = PARAM_ORDER;
	unsigned long batch = PARAM_BATCH;
	struct page **pages;		/* Pages that were allocated */
	int i, j;
	unsigned long long start_cycles_alloc, cycles_alloc;
	unsigned long long start_cycles_free, cycles_free;

	/* Allocate memory to store pointers to pages */
	pages = __vmalloc((PARAM_BATCH+1) * sizeof(struct page **),
			GFP_KERNEL|__GFP_HIGHMEM,
			PAGE_KERNEL);
	if (pages == NULL) {
		seq_printf(m, "Failed to allocate space to store page pointers\n");
		return -EINVAL;
	}
	memset(pages, 0, (PARAM_BATCH+1) * sizeof(struct page **));

	cycles_alloc = cycles_free = 0;

	for (i = 0; i < PARAM_ITERATIONS_OUTER; i++) {
		for (j = 0; j < PARAM_ITERATIONS_INNER; j++) {
			int nr_pages;

			/* No point hogging the CPU */
			cond_resched();

			/* Time allocations */
			start_cycles_alloc = read_clockcycles();
			for (nr_pages = 0; nr_pages <= batch; nr_pages++)
				pages[nr_pages] = alloc_pages(PARAM_GFPFLAGS | __GFP_NOWARN, order);
			cycles_alloc += read_clockcycles() - start_cycles_alloc;

			/* Time frees */
			start_cycles_free = read_clockcycles();
			for (nr_pages = 0; nr_pages <= batch; nr_pages++)
				if (pages[nr_pages] != NULL)
					__free_pages(pages[nr_pages], order);
			cycles_free += read_clockcycles() - start_cycles_free;
		}

		cycles_alloc = (unsigned long)cycles_alloc / PARAM_ITERATIONS_INNER;
		cycles_free = (unsigned long)cycles_free / PARAM_ITERATIONS_INNER;
		cycles_alloc = (unsigned long)cycles_alloc / batch;
		cycles_free = (unsigned long)cycles_free / batch;

		seq_printf(m, "order %2u batch %6lu alloc %llu free %llu\n",
			order, batch, cycles_alloc, cycles_free);
	}

	vfree(pages);

	return 0;
}

static int
pagealloc_open(struct inode *inode, struct file *file)
{
    return single_open(file, pagealloc_run, NULL);
}

static const struct file_operations pagealloc_fops = {
    .owner      = THIS_MODULE,
    .open       = pagealloc_open,
    .read       = seq_read,
    .llseek     = seq_lseek,
    .release    = single_release,
};
%}

function pagealloc_init() %{
    printk(KERN_INFO "Loading pagealloc microbenchmark\n");
    proc_create("mmtests-pagealloc-micro", 0, NULL, &pagealloc_fops);
%}

function pagealloc_exit() %{
    remove_proc_entry("mmtests-pagealloc-micro", NULL);
    printk(KERN_INFO "Unloading pagealloc microbenchmark.\n");
%}

probe begin
{
	pagealloc_init()
}

probe end
{
	pagealloc_exit()
}
